{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math, pickle, os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from utils import *\n",
    "import sys\n",
    "import models.nocond as nc\n",
    "import models.vqvae as vqvae\n",
    "import models.wavernn1 as wr\n",
    "import utils.env as env\n",
    "import argparse\n",
    "import platform\n",
    "import re\n",
    "import utils.logger as logger\n",
    "import time\n",
    "import subprocess\n",
    "\n",
    "import config\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Train or run some neural net')\n",
    "parser.add_argument('--generate', '-g', action='store_true')\n",
    "parser.add_argument('--embedding', '-e', action='store_true')\n",
    "parser.add_argument('--float', action='store_true')\n",
    "parser.add_argument('--half', action='store_true')\n",
    "parser.add_argument('--load', '-l')\n",
    "parser.add_argument('--scratch', action='store_true')\n",
    "parser.add_argument('--model', '-m')\n",
    "parser.add_argument('--force', action='store_true', help='skip the version check')\n",
    "parser.add_argument('--count', '-c', type=int, default=3, help='size of the test set')\n",
    "parser.add_argument('--partial', action='append', default=[], help='model to partially load')\n",
    "parser.add_argument('--single', default=None, help='single file for generation or embedding')\n",
    "parser.add_argument('--folder', default=None, help='folder for generation or embedding')\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "if args.float and args.half:\n",
    "    sys.exit('--float and --half cannot be specified together')\n",
    "\n",
    "if args.float:\n",
    "    use_half = False\n",
    "elif args.half:\n",
    "    use_half = True\n",
    "else:\n",
    "    use_half = False\n",
    "\n",
    "model_type = args.model or 'vqvae'\n",
    "\n",
    "model_name = f'{model_type}.43.upconv'\n",
    "\n",
    "\n",
    "if model_type == 'vqvae':\n",
    "    model_fn = lambda dataset: vqvae.Model(rnn_dims=896, fc_dims=896, global_decoder_cond_dims=dataset.num_speakers(),\n",
    "                  upsample_factors=(4, 4, 4), normalize_vq=True, noise_x=True, noise_y=True).cuda()\n",
    "    dataset_type = 'multi'\n",
    "elif model_type == 'wavernn':\n",
    "    model_fn = lambda dataset: wr.Model(rnn_dims=896, fc_dims=896, pad=2,\n",
    "                  upsample_factors=(4, 4, 4), feat_dims=80).cuda()\n",
    "    dataset_type = 'single'\n",
    "elif model_type == 'nc':\n",
    "    model_fn = lambda dataset: nc.Model(rnn_dims=896, fc_dims=896).cuda()\n",
    "    dataset_type = 'single'\n",
    "else:\n",
    "    sys.exit(f'Unknown model: {model_type}')\n",
    "\n",
    "if dataset_type == 'multi':\n",
    "    data_path = config.multi_speaker_data_path\n",
    "    with open(f'{data_path}/index.pkl', 'rb') as f:\n",
    "        index = pickle.load(f)\n",
    "    test_index = [x[-1:] if i < 2 * args.count else [] for i, x in enumerate(index)]\n",
    "    train_index = [x[:-1] if i < args.count else x for i, x in enumerate(index)]\n",
    "    dataset = env.MultispeakerDataset(train_index, data_path)\n",
    "elif dataset_type == 'single':\n",
    "    data_path = config.single_speaker_data_path\n",
    "    with open(f'{data_path}/dataset_ids.pkl', 'rb') as f:\n",
    "        index = pickle.load(f)\n",
    "    test_index = index[-args.count:] + index[:args.count]\n",
    "    train_index = index[:-args.count]\n",
    "    dataset = env.AudiobookDataset(train_index, data_path)\n",
    "else:\n",
    "    raise RuntimeError('bad dataset type')\n",
    "\n",
    "print(f'dataset size: {len(dataset)}')\n",
    "\n",
    "model = model_fn(dataset)\n",
    "\n",
    "if use_half:\n",
    "    model = model.half()\n",
    "\n",
    "for partial_path in args.partial:\n",
    "    model.load_state_dict(torch.load(partial_path), strict=False)\n",
    "\n",
    "paths = env.Paths(model_name, data_path)\n",
    "\n",
    "if args.scratch or args.load == None and not os.path.exists(paths.model_path()):\n",
    "    # Start from scratch\n",
    "    step = 0\n",
    "else:\n",
    "    if args.load:\n",
    "        prev_model_name = re.sub(r'_[0-9]+$', '', re.sub(r'\\.pyt$', '', os.path.basename(args.load)))\n",
    "        prev_model_basename = prev_model_name.split('_')[0]\n",
    "        model_basename = model_name.split('_')[0]\n",
    "        if prev_model_basename != model_basename and not args.force:\n",
    "            sys.exit(f'refusing to load {args.load} because its basename ({prev_model_basename}) is not {model_basename}')\n",
    "        if args.generate:\n",
    "            paths = env.Paths(prev_model_name, data_path)\n",
    "        prev_path = args.load\n",
    "    else:\n",
    "        prev_path = paths.model_path()\n",
    "        print(\"default loading model from \", prev_path)\n",
    "    step = env.restore(prev_path, model)\n",
    "\n",
    "#model.freeze_encoder()\n",
    "\n",
    "optimiser = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if args.generate:\n",
    "    if args.single is None:\n",
    "        model.do_generate(paths, step, data_path, test_index, use_half=use_half, verbose=True)#, deterministic=True)\n",
    "    else:\n",
    "        print(\"generating single: \", args.single)\n",
    "        model.do_generate_single(paths, step, args.single, 1, use_half=use_half, verbose=True)\n",
    "\n",
    "elif args.embedding:\n",
    "    if args.single is not None:\n",
    "        emb = model.do_single_embedding(paths, step, args.single, 1, use_half=use_half, verbose=True)\n",
    "        np.save(args.single.replace('.wav', '_embedding.npy'), emb)\n",
    "\n",
    "    elif args.folder:\n",
    "        print(\"generating folder embedding!\")\n",
    "\n",
    "        for subdir, dirs, files in os.walk(args.folder):\n",
    "            for file in files:\n",
    "                if not file.endswith(\"wav\"):\n",
    "                    continue\n",
    "\n",
    "                fullpath = os.path.join(subdir, file)\n",
    "                emb = model.do_single_embedding(paths, step, fullpath, 1, use_half=use_half, verbose=True)\n",
    "                dest = fullpath.replace(args.folder, args.folder+\"_embeddings\").replace('.wav', '.npy')\n",
    "                print(dest)\n",
    "\n",
    "                dest_folder = '/'.join(dest.split('/')[:-1])\n",
    "\n",
    "                if not os.path.exists(dest_folder):\n",
    "                    os.makedirs(dest_folder)\n",
    "\n",
    "                np.save(dest, emb)\n",
    "\n",
    "\n",
    "else:\n",
    "    logger.set_logfile(paths.logfile_path())\n",
    "    logger.log('------------------------------------------------------------')\n",
    "    logger.log('-- New training session starts here ------------------------')\n",
    "    logger.log(time.strftime('%c UTC', time.gmtime()))\n",
    "    model.do_train(paths, dataset, optimiser, epochs=1000, batch_size=16, step=step, lr=1e-4, use_half=use_half, valid_index=test_index)\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "${KERNEL_SPEC_DISPLAY_NAME}",
   "language": "${KERNEL_SPEC_LANGUAGE}",
   "name": "${KERNEL_SPEC_NAME}"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}